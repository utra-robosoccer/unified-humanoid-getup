unified-humanoid-get-up-env-standup-v0-10ms:
  n_timesteps: !!float 30e6
  policy: 'MlpPolicy'
  learning_rate: lin_3e-4
  buffer_size: 2500000
  batch_size: 512
  n_envs: 16
  gamma: 0.998
  ent_coef: 'auto'
  train_freq: 1
  gradient_steps: 1
  learning_starts: 10000
  policy_kwargs: "dict(
    net_arch=[384, 256],
    activation_fn=nn.ELU,
    optimizer_class=th.optim.AdamW,
    optimizer_kwargs=dict(weight_decay=1e-3)
  )"


unified-humanoid-get-up-env-standup-v0-25ms:
  n_timesteps: !!float 20e6
  policy: 'MlpPolicy'
  learning_rate: lin_3e-4
  buffer_size: 2000000
  batch_size: 512
  n_envs: 8
  gamma: 0.995
  ent_coef: 'auto'
  train_freq: 1
  gradient_steps: 1
  learning_starts: 1000
  policy_kwargs: "dict(
    net_arch=[384, 256],
    activation_fn=nn.ELU,
    optimizer_class=th.optim.AdamW,
    optimizer_kwargs=dict(weight_decay=1e-3)
  )"

unified-humanoid-get-up-env-standup-v0:
  n_timesteps: !!float 15e6
  policy: 'MlpPolicy'
  learning_rate: lin_3e-4
  buffer_size: 1000000
  batch_size: 256
  n_envs: 4
  gamma: 0.99
  ent_coef: 'auto'
  train_freq: 1
  gradient_steps: 1
  learning_starts: 10000
  policy_kwargs: "dict(
    net_arch=[384, 256],
    activation_fn=nn.ELU,
    optimizer_class=th.optim.AdamW,
    optimizer_kwargs=dict(weight_decay=1e-3)
  )"

unified-humanoid-get-up-env-standup-v0-vt:
  n_timesteps: !!float 20e6
  policy: 'MlpPolicy'
  learning_rate: lin_3e-4
  buffer_size: 2000000
  batch_size: 512
  n_envs: 8
  gamma: 0.998
  variable_time: True
  ent_coef: 'auto'
  train_freq: 1
  gradient_steps: 1
  learning_starts: 1000
  policy_kwargs: "dict(
    net_arch=[384, 256],
    activation_fn=nn.ELU,
    optimizer_class=th.optim.AdamW,
    optimizer_kwargs=dict(weight_decay=1e-3)
  )"
